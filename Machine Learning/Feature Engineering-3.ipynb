{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering-3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1. What is Min-Max scaling, and how is it used in data preprocessing? Provide an example to illustrate its application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Min-Max scaling, also known as normalization, is a data preprocessing technique used to scale numerical features within a specific range, typically between 0 and 1. This transformation is performed to ensure that all features have the same scale, making them more comparable and preventing certain features from dominating others when training machine learning models. Min-Max scaling is defined by the following formula for each feature:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "$$X_{scaled} = \\frac{X - X_{min}} {X_{max} - X_{min}} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where:\n",
    "\n",
    "- $X_{scaled}$ is the scaled value of the feature \n",
    "- $X $ the original value of the feature.\n",
    "- $X_{min}$ is the minimum value of the feature in the dataset.\n",
    "- $X_{max}$ is the maximum value of the feature in the dataset.\n",
    "\n",
    "After applying Min-Max scaling, the values of the feature will fall within the range [0, 1]. If the minimum and maximum values of the original feature are known, this scaling ensures that the minimum value maps to 0, and the maximum value maps to 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's an example to illustrate Min-Max scaling:\n",
    "\n",
    "Suppose you have a dataset containing the ages of people and their corresponding incomes, and you want to scale both features using Min-Max scaling.\n",
    "\n",
    "Original Data:\n",
    "\n",
    "- Age (in years): [25, 40, 30, 35, 50]\n",
    "- Income (in thousands): [30, 60, 40, 50, 80]\n",
    "\n",
    "For Age:\n",
    "- $X_{min}$ = 25 (minimum age)\n",
    "- $X_{max}$ = 50 (maximum age)\n",
    "\n",
    "For Income:\n",
    "- $X_{min}$ = 30 (minimum income)\n",
    "- $X_{max}$ = 80 (maximum income)\n",
    "\n",
    "Now, let's scale the data:\n",
    "\n",
    "Scaled Age:\n",
    "- $X_{scaled} = \\frac{X - 25}{50 - 25}$ for each value in the Age feature.\n",
    "\n",
    "Scaled Income:\n",
    "- $X_{scaled} = \\frac{X - 30}{80 - 25}$ for each value in the Income feature.\n",
    "\n",
    "The scaled values will be between 0 and 1 for both Age and Income. This scaling allows you to compare and use these features in machine learning algorithms without one feature dominating the other due to differences in their original scales."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2. What is the Unit Vector technique in feature scaling, and how does it differ from Min-Max scaling? Provide an example to illustrate its application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Unit Vector technique, also known as Unit Vector scaling or Vector normalization, is a feature scaling method that scales numerical features to have a unit norm. It differs from Min-Max scaling in that it doesn't scale the data to a specific range like [0, 1] but instead scales the data such that the resulting vector has a Euclidean norm (L2 norm) of 1. The L2 norm of a vector is the square root of the sum of the squares of its components.\n",
    "\n",
    "The formula for scaling a feature using Unit Vector scaling is as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$X_{scaled} = \\frac{X}{||X||_{2}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "​\n",
    " \n",
    "\n",
    "Where:\n",
    "\n",
    "- $X_{scaled}$ is the scaled value of the feature $X$.\n",
    "- $X$ is the original value of the feature.\n",
    "- $||X||_{2}$ represents the L2 norm of the feature vector $X$\n",
    "\n",
    "Unit Vector scaling is often used in machine learning when the direction of the data vectors is more important than their actual magnitude. It is useful in cases where the magnitude of the features is not crucial, such as in some clustering or dimensionality reduction algorithms.\n",
    "\n",
    "Here's an example to illustrate Unit Vector scaling:\n",
    "\n",
    "Suppose you have a dataset with two features, representing the length and width of different objects. You want to scale these features using Unit Vector scaling.\n",
    "\n",
    "Original Data:\n",
    "\n",
    "Length (in centimeters): [5, 8, 3, 10, 6]\n",
    "Width (in centimeters): [2, 4, 1, 5, 3]\n",
    "\n",
    "To scale the data using Unit Vector scaling:\n",
    "\n",
    "1. Calculate the L2 norm for each data point:\n",
    " \n",
    "2. Scale each feature by dividing by its L2 norm:\n",
    "\n",
    "Scaled Length: $ \\frac{Length}{∥Length∥_{2}} $\n",
    "\n",
    "Scaled Width: $ \\frac{width}{∥width∥_{2}} $\n",
    "\n",
    "For example, for the first data point:\n",
    "\n",
    "- Length: 5\n",
    "- Width: 2\n",
    "\n",
    "Calculate the L2 norm:  $ ∥X∥_{2} = \\sqrt{5^{2} + 2^{2}}$\n",
    "\n",
    "Scaled Length: $ \\frac{5}{\\sqrt{29}} $\n",
    "\n",
    "Scaled Length: $ \\frac{2}{\\sqrt{29}} $\n",
    "\n",
    "The scaled values will have a Euclidean norm of 1, meaning they fall on the unit circle in a two-dimensional space. This scaling technique emphasizes the direction of the data vectors while maintaining their relative relationships. It does not enforce a specific range for the features, unlike Min-Max scaling.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3. What is PCA (Principle Component Analysis), and how is it used in dimensionality reduction? Provide an example to illustrate its application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Principal Component Analysis (PCA) is a dimensionality reduction technique commonly used in machine learning and statistics. Its goal is to transform high-dimensional data into a new coordinate system, capturing the most important information while minimizing information loss. PCA achieves this by identifying the principal components, which are the directions in the data that have the maximum variance.\n",
    "\n",
    "The steps involved in PCA are as follows:\n",
    "\n",
    "1. **Standardize the Data**: Ensure that the data is centered (subtract the mean) and standardized (divide by the standard deviation) to give all features equal importance.\n",
    "\n",
    "2. **Compute the Covariance Matrix**: Calculate the covariance matrix for the standardized data.\n",
    "\n",
    "3. **Compute Eigenvectors and Eigenvalues**: Find the eigenvectors and eigenvalues of the covariance matrix. The eigenvectors represent the principal components, and the eigenvalues indicate the amount of variance captured by each principal component.\n",
    "\n",
    "4. **Sort and Select Principal Components**: Sort the eigenvectors by their corresponding eigenvalues in descending order. Choose the top $ k $ eigenvectors to form the new feature subspace (where $k$ is the desired dimensionality of the reduced data).\n",
    "\n",
    "5. **Project the Data**: Multiply the original standardized data by the selected eigenvectors to obtain the new reduced-dimensional data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Data:\n",
      " [[-0.51154143  1.31491696  0.64425338]\n",
      " [ 0.30841528 -0.73584035 -1.17636352]\n",
      " [-1.66932533  1.09676143  0.23057173]\n",
      " [ 0.70871634 -1.08533586  1.39625714]\n",
      " [ 1.16373513 -0.59050218 -1.09471874]]\n",
      "\n",
      "Reduced Data:\n",
      " [[ 1.42951997  0.13831095]\n",
      " [-1.09204359 -0.83910405]\n",
      " [ 1.92069255 -0.42795863]\n",
      " [-0.71671805  1.755521  ]\n",
      " [-1.54145089 -0.62676928]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Generate sample data\n",
    "np.random.seed(42)\n",
    "data = np.random.rand(5, 3)  # 5 samples, 3 features\n",
    "\n",
    "# Step 1: Standardize the data\n",
    "mean = np.mean(data, axis=0)\n",
    "std_dev = np.std(data, axis=0)\n",
    "standardized_data = (data - mean) / std_dev\n",
    "\n",
    "# Step 2: Compute the Covariance Matrix\n",
    "cov_matrix = np.cov(standardized_data, rowvar=False)\n",
    "\n",
    "# Step 3: Compute Eigenvectors and Eigenvalues\n",
    "eigenvalues, eigenvectors = np.linalg.eig(cov_matrix)\n",
    "\n",
    "# Step 4: Sort and Select Principal Components\n",
    "sorted_indices = np.argsort(eigenvalues)[::-1]\n",
    "top_k_indices = sorted_indices[:2]  # Select the top 2 principal components\n",
    "\n",
    "# Step 5: Project the Data\n",
    "pca = PCA(n_components=2)\n",
    "reduced_data = pca.fit_transform(standardized_data)\n",
    "\n",
    "# Print the original and reduced data\n",
    "print(\"Original Data:\\n\", standardized_data)\n",
    "print(\"\\nReduced Data:\\n\", reduced_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q4. What is the relationship between PCA and Feature Extraction, and how can PCA be used for Feature Extraction? Provide an example to illustrate this concept."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Principal Component Analysis (PCA) and Feature Extraction are closely related concepts used in data analysis and machine learning.\n",
    "\n",
    "PCA is a statistical procedure that uses an orthogonal transformation to convert a set of correlated variables into a set of uncorrelated variables1. It’s a technique for dimensionality reduction that identifies a set of orthogonal axes, called principal components, that capture the maximum variance in the data1. The main goal of PCA is to reduce the dimensionality of a dataset while preserving the most important patterns or relationships between the variables without any prior knowledge of the target variables1.\n",
    "\n",
    "On the other hand, Feature Extraction is a process of dimensionality reduction by which an initial set of raw data is reduced to more manageable groups for processing2. It’s the name for methods that select and/or combine variables into features, effectively reducing the amount of data that must be processed, while still accurately and completely describing the original data set2.\n",
    "\n",
    "So, PCA is actually a type of feature extraction technique. It’s used to reduce the dimensionality of a data set by finding a new set of variables, smaller than the original set of variables, retaining most of the sample’s information, and useful for the regression and classification of data1.\n",
    "\n",
    "For example, consider a dataset with a large number of features. If we apply PCA, it will identify a new set of variables, smaller than the original set of variables, that retains most of the sample’s information1. These new features, called Principal Components, are ordered in decreasing order of importance1. Here, Principal Component-1 (PC1) captures the maximum information of the original dataset, followed by PC2, then PC3, and so on1. This way, PCA helps in reducing the dimensionality of the dataset while preserving the most important patterns or relationships between the variables1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q5. You are working on a project to build a recommendation system for a food delivery service. The dataset contains features such as price, rating, and delivery time. Explain how you would use Min-Max scaling to preprocess the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q6. You are working on a project to build a model to predict stock prices. The dataset contains many features, such as company financial data and market trends. Explain how you would use PCA to reduce the dimensionality of the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q7. For a dataset containing the following values: [1, 5, 10, 15, 20], perform Min-Max scaling to transform the values to a range of -1 to 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q8. For a dataset containing the following features: [height, weight, age, gender, blood pressure], perform Feature Extraction using PCA. How many principal components would you choose to retain, and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
